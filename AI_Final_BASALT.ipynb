{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Randyflourish/Intro2AI-Final/blob/main/AI_Final_BASALT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxwl_yG1qhR7"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "  <img src=\"https://github.com/KarolisRam/MineRL2021-Intro-baselines/blob/main/img/colab_banner.png?raw=true\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BGlQwngccr"
      },
      "source": [
        "# Introduction\n",
        "This notebook is the installation part for the [MineRL 2022](https://minerl.io/) competition, building on the original introductory notebooks created for the MineRL 2021 competition.\n",
        "\n",
        "## Note: About this file\n",
        "\n",
        "This file is updated by NYCU 2024 Spring Intro2AI Team 11: まふまふ.\n",
        "The original file is come from [here](https://colab.research.google.com/drive/1rJ3lGy-bG7kJRe_wYBWg7fjSaD9oOMDw?usp=sharing)\n",
        "\n",
        "## There's a video to explain...\n",
        "Please visit [this intro YouTube video](https://youtu.be/8yIrWcyWGek) to see some background information.  Hopefully, this will lead to a number of additional videos that explore what can be done in this environment...\n",
        "\n",
        "And if you see me=@mdda online, then please say \"Hi!\"\n",
        "\n",
        "## Software 2.0\n",
        "The approach we are going to use, where we took some human written code and replaced it with an AI component is quite similar to how Tesla approaches self driving cars. See this talk by Andrej Karpathy, Director of AI at Tesla:  \n",
        "[Building the Software 2.0 Stack](https://databricks.com/session/keynote-from-tesla)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysSTXmT3YUeF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HTScYNljgXv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server python-opengl ffmpeg\n",
        "# Takes ~1min to run this\n",
        "# New Add\n",
        "!sudo apt-get install -y xvfb  # Install Xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh6gb3UWjT3p"
      },
      "outputs": [],
      "source": [
        "# This takes ~22mins - which would hit us every time we start Colab\n",
        "#   So we'll do it once, and store a '.tar.gz' of the installation into our\n",
        "#   Google Drive, so that we can get it back much quicker the second time!\n",
        "\n",
        "##%%capture\n",
        "##!pip3 install --upgrade minerl # Default is 0.4.4, we want 1.0.0 for VPT\n",
        "##!pip3 uninstall minerl\n",
        "#!pip3 install git+https://github.com/minerllabs/minerl@v1.0.0\n",
        "#\n",
        "#!pip3 install pyvirtualdisplay\n",
        "#!pip3 install -U colabgymrender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV7hXcp1vgj6"
      },
      "outputs": [],
      "source": [
        "import os, sys, time\n",
        "\n",
        "mine_env = 'mine_env'\n",
        "mine_env_full = f'/content/{mine_env}'\n",
        "mine_tar = f'{mine_env}.tar.gz'\n",
        "\n",
        "if mine_env_full not in sys.path:\n",
        "  sys.path.insert(0, mine_env_full)\n",
        "  os.environ['PYTHONPATH'] += f':{mine_env_full}'\n",
        "\n",
        "mine_env, mine_env_full, mine_tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvUh7yQ36RjQ"
      },
      "outputs": [],
      "source": [
        "# We'll connect to our Google Drive here, and see whether we've already saved off a copy\n",
        "#   This will ask permission to 'connect to your drive' : The answer is 'Yes'!\n",
        "MINE_ENV_IS_NEW = True\n",
        "\n",
        "from google.colab import drive  # google.colab contains functions specifically for interacting with Google Colab's environment.\n",
        "drive.mount('/content/drive')    # mounts your Google Drive as a local file system\n",
        "if os.path.isfile(f'/content/drive/MyDrive/pythonLib/{mine_tar}'): # check if \"mine_env.tar.gz\" is in your Google Drive\n",
        "  ! cp /content/drive/MyDrive/pythonLib/$mine_tar ./$mine_tar  # ! means the command is to be executed in the shell rather than as Python code.\n",
        "                                              # This command copies the file from your Google Drive to the current working directory of the Colab notebook.\n",
        "\n",
        "  ! ls -l ./$mine_tar                         # This lists the file details such as permissions, owner, size, and modification date for the copied file in the current directory.\n",
        "                                              # It helps verify that the file has been copied correctly and shows its properties.\n",
        "  # e.g.: -rw------- 1 root root 1510118446 Jun 26 08:48 ./mine_env.tar.gz\n",
        "\n",
        "  # ! tar -tzf ./$mine_tar | grep minerl | head -5    # list some contents of the compressed tar file without extracting it\n",
        "  ! tar -xzf ./$mine_tar    # This extracts the contents of the tar file into the current directory\n",
        "\n",
        "  MINE_ENV_IS_NEW = False\n",
        "  # Takes 1min too (huge saving!)\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/pythonLib')\n",
        "sys.path.append('/content/drive/MyDrive/pythonLib/VPT')\n",
        "\n",
        "\"DONE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdFxcyLg2DqL"
      },
      "outputs": [],
      "source": [
        "# Check default packages (execute if needed)\n",
        "!pip3 list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7ZYcRvnvq6y"
      },
      "outputs": [],
      "source": [
        "# Build the mine_env if necessary\n",
        "try:\n",
        "  from pyvirtualdisplay import Display\n",
        "except :\n",
        "  !pip3 install --target=$mine_env git+https://github.com/minerllabs/minerl@v1.0.2   # 21 mins\n",
        "  # https://stackoverflow.com/questions/55833509/attributeerror-type-object-callable-has-no-attribute-abc-registry\n",
        "  !mv $mine_env/typing.py $mine_env/MEH-typing.py  # Fix for Python3.7 ...\n",
        "\n",
        "  !pip3 install --target=$mine_env pyvirtualdisplay  # 4 secs  #注 Display creates a virtual framebuffer that graphical applications can use to render output as if they were using a real monitor.\n",
        "                                                              #注 This allows you to run applications that require a GUI without having an actual GUI environment installed on the system.\n",
        "  !pip3 install --target=$mine_env --upgrade colabgymrender # 22 secs  #注 colabgymrender provides a workaround by capturing the graphical output of the environment and displaying it within the notebook.\n",
        "\n",
        "  MINE_ENV_IS_NEW = True\n",
        "  # NB: some restart notices in the output ... but there's no need to restart!\n",
        "  #     In any case, please wait for the 'DONE' message to print out\n",
        "f\"DONE, with MINE_ENV_IS_NEW={MINE_ENV_IS_NEW}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNj2G3933yVu"
      },
      "outputs": [],
      "source": [
        "# check content of mine_env (execute if needed)\n",
        "! du -b mine_env | tail -5  # mine_env = ~ 2,094,031,775 bytes overall (a little bit less)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOxO92nU4C8Q"
      },
      "outputs": [],
      "source": [
        "# Build new env.tar.gz file in google drive (execute if needed)\n",
        "if MINE_ENV_IS_NEW: #  or True\n",
        "  # ! ls -l /gdrive/MyDrive/mine*\n",
        "  ! rm -f ./$mine_tar   #注 removes the existing tar.gz archive of the environment, if any, from the current directory.\n",
        "  ! tar -czf ./$mine_tar $mine_env  #注 This command creates a new compressed (gzipped) tar archive of the directory specified by the $mine_env variable (the environment directory).\n",
        "  ! ls -l ./$mine_tar\n",
        "  # Without running the env...\n",
        "  # -rw-r--r-- 1 root root 1505020174 Jun 26 07:26 ./mine_env.tar.gz\n",
        "  # Once the minerl env has been reset once (i.e. java has built...)\n",
        "  # -rw------- 1 root root 1511976116 Jun 26 08:43 ./mine_env.tar.gz\n",
        "  ! tar -tzf ./$mine_tar | head\n",
        "  ! cp ./$mine_tar /content/drive/MyDrive/pythonLib/  #注 This copies the newly created archive to a Google Drive directory.\n",
        "  ! ls -l /content/drive/MyDrive/pythonLib/$mine_tar\n",
        "\"DONE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmrUKxvYXGa"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8_vZpMFpiD9"
      },
      "outputs": [],
      "source": [
        "import os   # For interacting with the operating system.\n",
        "\n",
        "import numpy as np  # For numerical operations.\n",
        "\n",
        "import gym    # To create and manage environments based on the OpenAI Gym toolkit.\n",
        "import minerl\n",
        "\n",
        "from tqdm.notebook import tqdm  # For displaying progress bars in Jupyter notebooks.\n",
        "from colabgymrender.recorder import Recorder # To facilitate rendering of Gym environments in Google Colab.\n",
        "from pyvirtualdisplay import Display # To create a virtual display to render environments in a headless server or environment like Google Colab.\n",
        "\n",
        "import logging\n",
        "logging.disable(logging.ERROR) # reduce clutter, remove if something doesn't work to see the error logs.\n",
        "\n",
        "np.__version__  # '1.21.6' => that this is reading from our ~/mine_env directory\n",
        "# Numpy version may be different from the content above\n",
        "# About warning: since warning is in a local package, so if error occurs, please comment the specific line\n",
        "\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#from PIL import Image\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QwEA7yJ6Ail"
      },
      "source": [
        "# Download Dataset\n",
        "download a number of BASALT find-cave dataset, whose is specified in 'find-cave-Jul-28.json'\n",
        "\n",
        "Downloaded data includes the video data(.mp4), but also the corresponding actions(.jsonl)\n",
        "\n",
        "Target directory, which saves all the downloaded data, is /content/MineRLBasaltFindCave-v0 (in the disk)\n",
        "\n",
        "function 'download_file()' is refer to 'utils/download_dataset.py' in [basalt-2022-behavioural-cloning-baseline](https://github.com/minerllabs/basalt-2022-behavioural-cloning-baseline) with a little change\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0u4GNgalm8B"
      },
      "outputs": [],
      "source": [
        "from download_dataset import download_file\n",
        "download_file(2) # default is 400, about 40 GB?\n",
        "# !ls /content/MineRLBasaltFindCave-v0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjkM-WKjCSGx"
      },
      "source": [
        "# Construct Inverse Dynamic Model Agent\n",
        "load the 4x_idm model and load 4x_idm weight into the model\n",
        "\n",
        "function 'load_IDM_agent()' is refer to 'main() in ./VPT/run_inverse_dynamics_model.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehehsdr8CaG-"
      },
      "outputs": [],
      "source": [
        "from inverse_dynamics_model import load_IDM_agent\n",
        "IDMAgent = load_IDM_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the IDM\n",
        "check if the prediction of IDM is almost the same as ground truth actions\n",
        "\n",
        "also refer to 'main() in ./VPT/run_inverse_dynamics_model.py'\n"
      ],
      "metadata": {
        "id": "z8TaoZR8aCg7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2aHrFCHrMm"
      },
      "outputs": [],
      "source": [
        "# Test for IDMAgent\n",
        "import json\n",
        "import glob\n",
        "from run_inverse_dynamics_model import json_action_to_env_action\n",
        "from agent import ENV_KWARGS # need to modify\n",
        "required_resolution = ENV_KWARGS[\"resolution\"] # video required resolution\n",
        "files = glob.glob(\"/content/MineRLBasaltFindCave-v0/*.mp4\")\n",
        "video_path = files[0] # pick the first video to test\n",
        "json_path = video_path.replace(\".mp4\", \".jsonl\")\n",
        "\n",
        "cap = cv2.VideoCapture(video_path) # create a video capture object that can be used to read video files or streams\n",
        "\n",
        "json_index = 0\n",
        "with open(json_path) as json_file:\n",
        "  json_lines = json_file.readlines()\n",
        "  json_data = \"[\" + \",\".join(json_lines) + \"]\"\n",
        "  json_data = json.loads(json_data)\n",
        "\n",
        "# can be modified\n",
        "n_frames = 5  # how many frames of the video are loaded and processed in a single batch.\n",
        "          # Processing a specific number of frames at a time, instead of loading an entire video into memory, helps manage memory usage and computational load.\n",
        "# can be modified\n",
        "n_batches = 4  # This term describes the total number of these batches that will be processed during the script’s run\n",
        "          # If the video has more frames, they won't be processed unless the n_batches count is increased.\n",
        "for _ in range(n_batches):\n",
        "  # import torch as th\n",
        "  # th.cuda.empty_cache() # clear the unused memory from the GPU's cache\n",
        "  print(\"=== Loading up frames ===\")\n",
        "  frames = []\n",
        "  recorded_actions = []\n",
        "  for _ in range(n_frames):\n",
        "    ret, frame = cap.read() # capture a frame from the video, 'ret' is a boolean indicating if the frame was successfully read\n",
        "    if not ret: # end of the video\n",
        "      break\n",
        "    assert frame.shape[0] == required_resolution[1] and frame.shape[1] == required_resolution[0], \"Video must be of resolution {}\".format(required_resolution)\n",
        "    # BGR -> RGB\n",
        "    frames.append(frame[..., ::-1]) # Converts the frame from BGR (Blue, Green, Red - default color format in OpenCV) to RGB format and stores it in the frames list\n",
        "    env_action, _ = json_action_to_env_action(json_data[json_index]) # extract the ground truth action\n",
        "    recorded_actions.append(env_action)\n",
        "    json_index += 1\n",
        "  frames = np.stack(frames) # stacks the list of frames into a numpy array, making it suitable for batch processing in the model prediction\n",
        "  print(\"=== Predicting actions ===\")\n",
        "  predicted_actions = IDMAgent.predict_actions(frames) # pass a batch of frames into IDM, and get the output actions\n",
        "\n",
        "  for i in range(len(frames)):\n",
        "    for y, (action_name, action_array) in enumerate(predicted_actions.items()):\n",
        "      print(f\"{action_name}: {action_array[0, i]} ({recorded_actions[i][action_name]}), \", end = \"\")\n",
        "    print(\"\\n\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmalpPQeHy0X"
      },
      "outputs": [],
      "source": [
        "disp = Display(visible=0, backend=\"xvfb\") #註 set up a virtual display, allows the minecraft backend to writh to something\n",
        "                       #  even though colab does not have an actual display\n",
        "disp.start();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXMu7Ek5x6nl"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MineRLBasaltFindCave-v0\") #註 one of the environments: find a cave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7BT_7Ss9HBl"
      },
      "outputs": [],
      "source": [
        "env.action_space.sample().keys()  #註 action space that the minecraft agent can do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6HkWKSIJgL1"
      },
      "outputs": [],
      "source": [
        "# Have a look at a few actions we might do:\n",
        "for _ in range(10):\n",
        "  print( env.action_space.sample() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs9o7KW-tUcq"
      },
      "outputs": [],
      "source": [
        "t0=time.time()\n",
        "obs = env.reset()  # First obs is thrown away...\n",
        "            #註 reset the environment\n",
        "print(f\"{(time.time()-t0):.2f}sec for env.reset\")\n",
        "# 275.65sec = 4mins for first time, 80.73sec second time (due to compilation of java files?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3FGwWZ489mT"
      },
      "outputs": [],
      "source": [
        "# Now that Steve has been spawned, do some actions...\n",
        "t0=time.time()\n",
        "\n",
        "done, iter = False, 0\n",
        "while not done:\n",
        "    ac = env.action_space.noop() #註 'ac' is initialized with a no-operation action('noop')\n",
        "    # Spin around to see what is around us\n",
        "    ac[\"camera\"] = [0, 0]  # (pitch, yaw) deltas in degrees : +30 => turn to right\n",
        "    ac['attack'] = [1]\n",
        "\n",
        "    t1=time.time()\n",
        "    obs, reward, done, info = env.step(ac) #註 obs is 'next observation'\n",
        "    #print(obs, reward, info)  # NB: Yikes : obs is only the image!\n",
        "    #  obs = Dict(pov:Box(low=0, high=255, shape=(360, 640, 3)))\n",
        "    #print(pov.shape) # (360, 640, 3)  Image spec agrees with docs!\n",
        "    print(f\"{(time.time()-t1):.2f}sec for env.step\")  # Approx 0.25sec per step\n",
        "\n",
        "    pov = obs[\"pov\"] #註 pov(point of view) is an image which is extracted from the obs(observation)\n",
        "\n",
        "    #env.render()  # This does an internal cv2.imshow that colab rejects\n",
        "    #cv2_imshow(pov[:, :, ::-1])\n",
        "    #cv2.waitKey(1)\n",
        "\n",
        "    plt.imshow(pov) #註 在圖表中繪製圖片(繪製pov)\n",
        "    plt.show() #註 顯示圖表\n",
        "    iter +=1\n",
        "    if iter>12: done=True\n",
        "\n",
        "f\"{(time.time()-t0):.2f}sec for whole spin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytLkHyBow1Ct"
      },
      "outputs": [],
      "source": [
        "# Set up a simple testing function\n",
        "def action_step(action):\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(action)\n",
        "  obs, reward, done, info = env.step(ac)\n",
        "  plt.imshow(obs[\"pov\"])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9hUkcZhuObe"
      },
      "outputs": [],
      "source": [
        "action_step({})\n",
        "action_step(dict(inventory=[1]))\n",
        "action_step(dict(camera=[0, +30]))\n",
        "action_step(dict(camera=[-10, -30]))\n",
        "action_step(dict(camera=[+10, 0]))\n",
        "action_step(dict(inventory=[1]))  # Put inventory away? = Yes, if it is showing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taCclVGyuSuZ"
      },
      "outputs": [],
      "source": [
        "#action_step({'inventory':[1]})  # Put inventory away? = NOT jump, sneak, use, hotbar.X, back\n",
        "action_step({})  # NOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssp9BtYZ3u4g"
      },
      "outputs": [],
      "source": [
        "# Set up a simple calibration function\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def action_step_calibrate(x_off,y_off):\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(dict(camera=[y_off, x_off]))\n",
        "  obs, reward, done, info = env.step(ac)\n",
        "  im = obs[\"pov\"][100:250, 200:400,:]\n",
        "  cv2_imshow(cv2.cvtColor(im, cv2.COLOR_RGB2BGR))\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(dict(camera=[-y_off, -x_off]))  # Move back\n",
        "  obs, reward, done, info = env.step(ac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ung8m4tp3ueX"
      },
      "outputs": [],
      "source": [
        "action_step({})\n",
        "action_step(dict(inventory=[1]))\n",
        "\n",
        "action_step_calibrate(0, 0)\n",
        "for x_off in [+0.62, +1.61, +3.22, +5.81, +10.0]:\n",
        "  print(f\"x_off={x_off}\")\n",
        "  action_step_calibrate(x_off,0)\n",
        "  action_step_calibrate(-x_off,0)\n",
        "for y_off in [+0.62, +1.61, +3.22, +5.81, +10.0]:\n",
        "  print(f\"y_off={y_off}\")\n",
        "  action_step_calibrate(0, y_off)\n",
        "  action_step_calibrate(0, -y_off)\n",
        "\n",
        "action_step(dict(inventory=[1]))  # Put inventory away? = Yes, if it is showing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Hu5elxuluu"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvxBJAcA73fv"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERSdDSW61fnw"
      },
      "outputs": [],
      "source": [
        "disp.stop();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO__56dWX3en"
      },
      "outputs": [],
      "source": [
        "# THE END! - We'll be using this set-up in the future!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}